{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Побудова бейзлайнового чанкера на основі правил."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У нас уже є непоганий POS-таггер, тепер використаємо його для того, щоб витягати з речень іменникові та дієслівні фрази."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import nltk\n",
    "\n",
    "from tokenize_uk import tokenize_words, tokenize_sents\n",
    "from perceptron_tagger.tagger import PerceptronTagger\n",
    "tagger = PerceptronTagger()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`nltk.RegexpParser` дозволяє задати набір regexp-правил, які використовуються для знаходження чанків у тексті з роставленими тегами частин мови."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions\n",
    "def pos_chunk(sent, tagger, parser):\n",
    "    \"\"\"\n",
    "    Use tagger and chunk parser to create chunks.\n",
    "    \"\"\"\n",
    "    vps, nps  = [], []\n",
    "    pos_sent = tagger.tag(sent)\n",
    "    parsed_sent = parser.parse(pos_sent)\n",
    "    for s in parsed_sent.subtrees():\n",
    "        if s.label() == 'VP':\n",
    "            vps.append(s.leaves())\n",
    "        elif s.label() == 'NP':\n",
    "            nps.append(s.leaves())\n",
    "    res = {'sent': sent,\n",
    "           'VPs': vps,\n",
    "           'NPs': nps}\n",
    "    return res\n",
    "\n",
    "def print_phrases(res_dict):\n",
    "    \"\"\"\n",
    "    Prints res_dict from pos_chunk function.\n",
    "    \"\"\"\n",
    "    print('Sentence:')\n",
    "    sent = res_dict['sent']\n",
    "    vps = res_dict['VPs']\n",
    "    nps = res_dict['NPs']\n",
    "    print(sent)\n",
    "    if nps:\n",
    "        print('Noun phrases:')\n",
    "        print([' '.join([w[0] for w in np]) for np in nps])\n",
    "    if vps:\n",
    "        print('Verb phrases')\n",
    "        print([' '.join([w[0] for w in vp]) for vp in vps])\n",
    "    print('---')\n",
    "    \n",
    "def write_phrases(res_dict):\n",
    "    \"\"\"\n",
    "    Make a line for tsv file.\n",
    "    \"\"\"\n",
    "    sent = res_dict['sent']\n",
    "    vps = [' '.join([w[0] for w in vp]) for vp in res_dict['VPs']]\n",
    "    nps = [' '.join([w[0] for w in np]) for np in res_dict['NPs']]\n",
    "    lennps = len(nps)\n",
    "    lenvps = len(vps)\n",
    "    vps = ', '.join(vps)\n",
    "    nps = ', '.join(nps)\n",
    "    line = sent+'\\t'+nps+'\\t'+vps+'\\t'+str(lennps)+'\\t'+str(lenvps)+'\\n'\n",
    "    return line\n",
    "\n",
    "def phrases_to_file(list_of_dicts, file_path):\n",
    "    \"\"\"\n",
    "    Make a tsv file.\n",
    "    \"\"\"\n",
    "    lines = [write_phrases(res) for res in list_of_dicts]\n",
    "    first = 'sentence\\tNoun phrases\\tVerb phrases\\tn(NP)\\tn(VP)\\n'\n",
    "    lines.insert(0, first)\n",
    "    with open(file_path, 'w') as f:\n",
    "        f.writelines(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create grammar and initialize chunk parser\n",
    "grammar = r\"\"\"\n",
    "NP: {<DET><ADJ><NOUN>+}\n",
    "    {<DET>*<ADJ><NOUN>+}\n",
    "    {<DET><ADJ>*<NOUN>+}\n",
    "    {<DET><ADJ><NOUN>*<PROPN>*}\n",
    "    {<DET>*<ADJ>*<NOUN>+<ADJ>*<NOUN|PROPN>+}\n",
    "    {<NOUN><NOUN|PROPN>(<CCONJ><NOUN|PROPN>)?}\n",
    "    {<DET>*(<ADJ>|<ADJ><CCONJ><ADJ>)*(<NOUN|PROPN><CCONJ><NOUN|PROPN>|<NOUN|PROPN>)+}\n",
    "VP: {<ADV>*<VERB>+}\n",
    "    {<AUX>+<VERB>+}\n",
    "\"\"\"\n",
    "cp = nltk.RegexpParser(grammar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для бейзлайнової питально-відповідальної системи я створював набір тестових питань про географічні об'єкти. Можна перевірити роботу чанкера на них."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('QA/test_questions.txt') as f:\n",
    "    raw = f.read().strip()\n",
    "    questions = [q.strip() for q in raw.split('\\n')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sent': 'яка густота населення Австралії',\n",
       " 'VPs': [],\n",
       " 'NPs': [[('яка', 'DET'), ('густота', 'NOUN'), ('населення', 'NOUN')],\n",
       "  [('Австралії', 'PROPN')]]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_chunk(questions[15], tagger, cp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У цьому випадку він знайшов дві іменникові фрази і жодної дієслівної (тому що в реченні їх і не було). \"Яка густота населення\" та \"Австралії\" - окремі фрази, тому що в перспективі для знаходження відповіді на це питання краще їх розділяти, і це враховано у правилах."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для зручнішого перегляду можна витягнути всі іменникові та дієслівні фрази у окремий файл:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_chunks = []\n",
    "for q in questions:\n",
    "    q_chunks.append(pos_chunk(q, tagger, cp))\n",
    "\n",
    "phrases_to_file(q_chunks, 'qchunks.tsv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Я \"протестував\" чанкер на цьому файлі. Очікувано, він витягає практично все, що можливо, з цих досить простих питань. Тільки у 5 питаннях із 128 чанкер не витягнув слова, без яких система не дала б відповідь на питання, тобто recall дуже високий. Водночас precision не дуже хороший - інколи одна смислова фраза розбита на дві або відношення і об'єкт опиняються в одній фразі."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можна створити аналогічний чанкер і на основі залежностей. Для прикладу можна взяти вже проанотовані речення."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import conllu\n",
    "import gzip\n",
    "\n",
    "fname = 'uk_iu-ud-dev.conllu.gz'\n",
    "with gzip.open(fname, 'rb') as f:\n",
    "    raw_dev = f.read().decode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev = conllu.parse(raw_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  Нині/advmod\n",
      "  зростає/root\n",
      "  (NP інтерес/nsubj)\n",
      "  до/case\n",
      "  (NP біоміметики/nmod)\n",
      "  як/mark\n",
      "  до/case\n",
      "  (NP галузі/appos копіювання/nmod)\n",
      "  (NP\n",
      "    унікальних/amod\n",
      "    функцій/nmod\n",
      "    і/cc\n",
      "    виробничих/amod\n",
      "    процесів/conj)\n",
      "  (NP живих/amod організмів/nmod)\n",
      "  ,/punct\n",
      "  застосування/conj\n",
      "  цих/det\n",
      "  (NP технологій/nmod)\n",
      "  під/case\n",
      "  (NP час/nmod розробки/nmod й/cc створення/conj)\n",
      "  (NP продукції/nmod)\n",
      "  ./punct)\n"
     ]
    }
   ],
   "source": [
    "def strip_colon(deprel):\n",
    "    if not ':' in deprel:\n",
    "        return deprel\n",
    "    else:\n",
    "        return deprel.split(':')[0]\n",
    "    \n",
    "grammar = r\"\"\"\n",
    "NP: {<amod><nsubj>(<cc><conj>)?}\n",
    "    {<nsubj><nmod><appos>?}\n",
    "    {<amod>?<nmod>+<cc><amod>?<conj>}\n",
    "    {<appos><nmod>(<cc><conj>)?}\n",
    "    {<det>?<amod>?<nmod><amod>?<nmod><flat>?}    \n",
    "    {<amod>?<nmod>+<cc><amod>?<conj>}\n",
    "    {<amod>*<nmod>+<flat>?(<cc><conj>)?}\n",
    "    {<amod>*<nsubj>+(<cc><conj>)?}\n",
    "    {<det>?<amod>+<obj>(<cc><conj>)?}\n",
    "    {<amod>?<obj>(<cc><conj>)?}\n",
    "    {<flat><cc><conj>}\n",
    "    {<det>?<amod>?<obl>(<cc><conj>)?}\n",
    "\"\"\"\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "sent = [(w['form'], strip_colon(w['deprel'])) for w in dev[35]]\n",
    "print(cp.parse(sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Звісно, правила для чанкінгу обох типів підлягають подальшому покращенню, хоча зрозуміло, що правила функціонуватимуть гірше, ніж натренована на даних програма - але цих даних якраз немає (і їх, можливо, потрібно створити)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
