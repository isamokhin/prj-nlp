{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Побудова POS-таггера методами машинного навчання."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У цьому ноутбуці буде використано два таггери: один побудований власноруч, на основі логістичної регресії; інший - адаптований із коду Метью Хонібала (https://github.com/sloria/textblob-aptagger/tree/master), на основі усередненого перцептрона. Обидві моделі дали схожі результати після тренування та тестування на відповідних вибірках з українського корпусу універсальних залежностей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import conllu\n",
    "import random\n",
    "from collections import OrderedDict, Counter\n",
    "import string\n",
    "import gzip\n",
    "\n",
    "from tokenize_uk import tokenize_words\n",
    "from ukr_stemmer3 import UkrainianStemmer\n",
    "import pymorphy2\n",
    "morph = pymorphy2.MorphAnalyzer(lang='uk')\n",
    "\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для тренування використовувався train, для тюнингу ознак і параметрів моделі - dev-вибірка. Тут для тестування я використаю тільки test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = 'uk_iu-ud-train.conllu.gz'\n",
    "with gzip.open(fname, 'rb') as f:\n",
    "    raw_train = f.read().decode()\n",
    "\n",
    "fname2 = 'uk_iu-ud-test.conllu.gz'\n",
    "with gzip.open(fname2, 'rb') as f2:\n",
    "    raw_test = f2.read().decode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = conllu.parse(raw_train)\n",
    "test_set = conllu.parse(raw_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(для фіч використано стеммер звідси: https://github.com/Amice13/ukr_stemmer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tagger:\n",
    "    \"\"\"\n",
    "    The POS-tagger based on logistic regression.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, load=True, model_loc='model.pkl'):\n",
    "        self.MODEL_LOC = model_loc\n",
    "        if load:\n",
    "            self.MODEL = self.load(self.MODEL_LOC)\n",
    "            \n",
    "    def extract_features(self, i, sentence):\n",
    "        \"\"\"\n",
    "        Feature extractor for the classifier.\n",
    "        Sentence is a list of words.\n",
    "        \"\"\"\n",
    "        stem = UkrainianStemmer(sentence[i]).stem_word()\n",
    "        suf = 'NONE' if len(stem) == len(sentence[i]) else sentence[i][len(stem):]\n",
    "        features = {\n",
    "            'word': sentence[i],\n",
    "            'word_stem': stem,\n",
    "            'word_suf': suf,\n",
    "            'prefix-3': sentence[i][:3],\n",
    "            'suffix-3': sentence[i][-3:],\n",
    "            'is_first': i == 0,\n",
    "            'is_last': i == len(sentence) - 1,\n",
    "            'has_hyphen': '-' in sentence[i],\n",
    "            'is_numeric': sentence[i].isdigit(),\n",
    "            'is_punct': sentence[i] in string.punctuation,\n",
    "            'is_all_caps': sentence[i].upper() == sentence[i],\n",
    "            'has_ascii': any(ord(char) < 128 for char in sentence[i])\n",
    "        }\n",
    "        if i == 0:\n",
    "            features.update({\n",
    "                'is_first': 1,\n",
    "                'word-1': '<START>'\n",
    "            })\n",
    "        if i == len(sentence) - 1:\n",
    "            features.update({\n",
    "                'is_last': 1,\n",
    "                'word+1': '<END>'\n",
    "            })\n",
    "        if i > 0:\n",
    "            features.update({\n",
    "                'is_capitalized': sentence[i][0].upper() == sentence[i][0],\n",
    "                'word-1': sentence[i-1],\n",
    "                'bigram-1': sentence[i-1]+'_'+sentence[i],\n",
    "                'word-1-suf': sentence[i-1][-3:]\n",
    "            })\n",
    "        if i > 1:\n",
    "            features.update({\n",
    "                'word-2': sentence[i-2],\n",
    "                'trigram-2': sentence[i-2] + '_' + sentence[i-1] + '_' + sentence[i],\n",
    "                'word-2-suf': sentence[i-2][-3:]\n",
    "            })\n",
    "        if i < len(sentence) - 1:\n",
    "            features.update({\n",
    "                'word+1': sentence[i+1],\n",
    "                'bigram+1': sentence[i]+'_'+sentence[i+1],\n",
    "                'word+1-suf': sentence[i+1][-3:]\n",
    "            })\n",
    "        if i < len(sentence) - 2:\n",
    "            features.update({\n",
    "                'word+2': sentence[i+2],\n",
    "                'trigram+2': sentence[i]+'_'+sentence[i+1]+'_'+sentence[i+2],\n",
    "                'word+2-suf': sentence[i+2][-3:]\n",
    "            })\n",
    "        return features\n",
    "    \n",
    "    def process_corpus(self, corpus):\n",
    "        \"\"\"\n",
    "        Get features and labels from the list of sentences,\n",
    "        where each sentence is a list of tuples.\n",
    "        \"\"\"\n",
    "        labels, features = [], []\n",
    "        for sent in corpus:\n",
    "            sent_words = [word[0] for word in sent]\n",
    "            for i in range(len(sent)):\n",
    "                labels.append(sent[i][1])\n",
    "                feat_dict = self.extract_features(i, sent_words)\n",
    "                # use previous tags in training\n",
    "                if i > 0:\n",
    "                    feat_dict.update({'pos-1': sent[i-1][1]})\n",
    "                if i > 1:\n",
    "                    feat_dict.update({'pos-2': sent[i-2][1]})\n",
    "                features.append(feat_dict)\n",
    "        return features, labels\n",
    "\n",
    "    def get_features_unlabeled(self, corpus):\n",
    "        \"\"\"\n",
    "        Get features from the list of sentences,\n",
    "        where each sentence is a list of tokens.\n",
    "        \"\"\"\n",
    "        features = []\n",
    "        for sent in corpus:\n",
    "            for i in range(len(sent)):\n",
    "                feat_dict = self.extract_features(i, sent)\n",
    "                features.append(feat_dict)\n",
    "        return features\n",
    "    \n",
    "    def make_corpus(self, data):\n",
    "        \"\"\"\n",
    "        Process data in CONLLU format.\n",
    "        \"\"\"\n",
    "        corpus = []\n",
    "        for sent in data:\n",
    "            corpus.append([(w['form'], w['upostag']) for w in sent])\n",
    "        return corpus\n",
    "    \n",
    "    def train_pipeline(self, train_features, train_labels, random_state=0, save=True):\n",
    "        \"\"\"\n",
    "        Train a model for tagging.\n",
    "        \"\"\"\n",
    "        vec = DictVectorizer()\n",
    "        clf = LogisticRegression(random_state=random_state, penalty='l1')\n",
    "        pipeline = Pipeline([('vec', vec), ('clf', clf)])\n",
    "        pipeline.fit(train_features, train_labels)\n",
    "        if save:\n",
    "            self.save(pipeline)\n",
    "        return pipeline\n",
    "    \n",
    "    def map_pos(self, word_parsed):\n",
    "        \"\"\"\n",
    "        Map between pymorphy2 and UD POS tags.\n",
    "        \"\"\"\n",
    "        MAPPING = {\"ADJF\": \"ADJ\", \"ADJS\": \"ADJ\", \"COMP\": \"ADJ\", \"PRTF\": \"ADJ\",\n",
    "               \"PRTS\": \"ADJ\", \"GRND\": \"VERB\", \"NUMR\": \"NUM\", \"ADVB\": \"ADV\",\n",
    "               \"PRED\": \"ADV\", \"PREP\": \"ADP\", \"PRCL\": \"PART\", \"NOUN\": \"NOUN\",\n",
    "               \"VERB\": \"VERB\", \"INTJ\": \"INTJ\"}\n",
    "        pm_tag = word_parsed.tag.POS\n",
    "        if not pm_tag:\n",
    "            return None\n",
    "        if pm_tag == \"CONJ\":\n",
    "            if \"coord\" in word_parsed.tag:\n",
    "                pos = \"CCONJ\"\n",
    "            else:\n",
    "                pos = \"SCONJ\"\n",
    "        else:\n",
    "            pos = MAPPING.get(pm_tag, None)\n",
    "        return pos\n",
    "        \n",
    "    def is_unambigous(self, word):\n",
    "        \"\"\"\n",
    "        For use in cases where there is only one possible POS.\n",
    "        \"\"\"\n",
    "        parsed_list = morph.parse(word)\n",
    "        if len(parsed_list) == 1:\n",
    "            return self.map_pos(parsed_list[0])\n",
    "        elif len(set(word.tag.POS for word in parsed_list)) == 1:\n",
    "            return self.map_pos(parsed_list[0])\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def classify_word(self, word_features, pipeline):\n",
    "        \"\"\"\n",
    "        Classify word using both tagger and pymorphy2.\n",
    "        \"\"\"\n",
    "        word = word_features['word']\n",
    "        if 'is_capitalized' in word_features and word_features['is_capitalized']:\n",
    "            return pipeline.predict([word_features])[0]\n",
    "        pm_tag = self.is_unambigous(word)\n",
    "        if pm_tag:\n",
    "            return pm_tag\n",
    "        else:\n",
    "            return pipeline.predict([word_features])[0]\n",
    "\n",
    "    def predict_tags(self, features, pipeline=None):\n",
    "        \"\"\"\n",
    "        Make a prediction for a list of features.\n",
    "        \"\"\"\n",
    "        if pipeline is None:\n",
    "            pipeline = self.MODEL\n",
    "        labels = []\n",
    "        for i, f in enumerate(features):\n",
    "            # use previous predicted tags \n",
    "            if 'word-1' in f and f['word-1'] != '<START>':\n",
    "                f.update({'pos-1': labels[i-1]})\n",
    "            if 'word-2' in f:\n",
    "                f.update({'pos-2': labels[i-2]})\n",
    "            labels.append(self.classify_word(f, pipeline))\n",
    "        return labels\n",
    "    \n",
    "    def save(self, pipeline, fname='model.pkl'):\n",
    "        \"\"\"\n",
    "        Save a fitted model for tagger.\n",
    "        \"\"\"\n",
    "        joblib.dump(pipeline, fname)\n",
    "        print('Model is saved to', fname)\n",
    "        \n",
    "    def load(self, fname='model.pkl'):\n",
    "        \"\"\"\n",
    "        Load a pretrained model.\n",
    "        \"\"\"\n",
    "        pipeline = joblib.load(fname)\n",
    "        return pipeline\n",
    "    \n",
    "    def tag_sentence(self, sent, pipeline=None):\n",
    "        \"\"\"\n",
    "        Tag a sentence with pretrained model.\n",
    "        Returns a list of tuples (word, tag).\n",
    "        \"\"\"\n",
    "        if pipeline is None:\n",
    "            pipeline = self.MODEL\n",
    "        sent_tokenized = tokenize_words(sent)\n",
    "        sent_features = self.get_features_unlabeled([sent_tokenized])\n",
    "        tags = self.predict_tags(sent_features, pipeline)\n",
    "        return list(zip(sent_tokenized, tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is saved to model.pkl\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vec', DictVectorizer(dtype=<class 'numpy.float64'>, separator='=', sort=True,\n",
       "        sparse=True)), ('clf', LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l1', random_state=477, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = Tagger(load=False)\n",
    "train_corpus = t.make_corpus(train_set)\n",
    "train_features, train_labels = t.process_corpus(train_corpus)\n",
    "t.train_pipeline(train_features, train_labels, random_state=477)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        ADJ      0.942     0.944     0.943      1723\n",
      "        ADP      0.996     0.987     0.991      1377\n",
      "        ADV      0.905     0.883     0.894       618\n",
      "        AUX      0.846     0.206     0.331       107\n",
      "      CCONJ      0.957     0.975     0.966       554\n",
      "        DET      0.956     0.897     0.925       552\n",
      "       INTJ      1.000     0.400     0.571        10\n",
      "       NOUN      0.960     0.961     0.960      4021\n",
      "        NUM      0.913     0.930     0.922       272\n",
      "       PART      0.905     0.814     0.857       306\n",
      "       PRON      0.900     0.910     0.905       423\n",
      "      PROPN      0.802     0.852     0.827       576\n",
      "      PUNCT      0.996     1.000     0.998      2677\n",
      "      SCONJ      0.804     0.944     0.869       231\n",
      "        SYM      1.000     0.375     0.545        16\n",
      "       VERB      0.932     0.985     0.957      1359\n",
      "          X      0.939     0.915     0.926       117\n",
      "\n",
      "avg / total      0.949     0.949     0.947     14939\n",
      "\n"
     ]
    }
   ],
   "source": [
    "t = Tagger()\n",
    "test_corpus = t.make_corpus(test_set)\n",
    "test_features, test_labels = t.process_corpus(test_corpus)\n",
    "pred_labels = t.predict_tags(test_features)\n",
    "print(classification_report(test_labels, pred_labels, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.949\n"
     ]
    }
   ],
   "source": [
    "print(round(accuracy_score(test_labels, pred_labels), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Точність таггера 95% - дуже непогано! Можна подивитись, де найбільше помиляється таггер:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('NOUN', 'PROPN'), 88),\n",
       " (('AUX', 'VERB'), 78),\n",
       " (('PROPN', 'NOUN'), 54),\n",
       " (('NOUN', 'ADJ'), 51),\n",
       " (('DET', 'PRON'), 36),\n",
       " (('ADV', 'NOUN'), 36),\n",
       " (('PROPN', 'ADJ'), 27),\n",
       " (('ADJ', 'PROPN'), 24),\n",
       " (('ADJ', 'ADV'), 24),\n",
       " (('ADJ', 'NOUN'), 24)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf_pairs = []\n",
    "test_words = [f['word'] for f in test_features]\n",
    "for w, l, p in zip(test_words, test_labels, pred_labels):\n",
    "    if l != p:\n",
    "        conf_pairs.append((l, p))\n",
    "\n",
    "Counter(conf_pairs).most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Плутанина між іменником та іменником-власною назвою - дуже зрозуміла, хоч і неприємна для перспектив NER. AUX i VERB плутати логічно, так само як DET i PRON, а ось плутанина між NOUN i ADJ менш прийнятна."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Перцептрон має переваги порівняно з попередньою моделлю, насамперед у швидкості та розмірі натренованої моделі. Для таггера я додав пару ознак плюс перевірку за словником pymorphy2 (тестування на реченнях, несхожих на тренувальний корпус, показало обмеженість натренованого словника перцептрона)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and saving the perceptron POS tagger.\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "from perceptron_tagger.tagger import PerceptronTagger\n",
    "\n",
    "def perceptron_train_and_save(train_corpus, nr_iter=10, \n",
    "                              fname='uk_perceptron_tagger.pickle'):\n",
    "    perc_train = []\n",
    "    for sent in train_corpus:\n",
    "        words = [w[0] for w in sent]\n",
    "        tags = [w[1] for w in sent]\n",
    "        perc_train.append((words, tags))\n",
    "    p = PerceptronTagger(load=False)\n",
    "    p.train(perc_train, nr_iter=nr_iter, save_loc=fname)\n",
    "    \n",
    "print('Training and saving the perceptron POS tagger.')\n",
    "perceptron_train_and_save(train_corpus, 20)\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        ADJ      0.937     0.954     0.945      1723\n",
      "        ADP      0.995     0.987     0.991      1377\n",
      "        ADV      0.908     0.909     0.909       618\n",
      "        AUX      0.815     0.206     0.328       107\n",
      "      CCONJ      0.962     0.960     0.961       554\n",
      "        DET      0.944     0.850     0.894       552\n",
      "       INTJ      0.500     0.300     0.375        10\n",
      "       NOUN      0.953     0.964     0.959      4021\n",
      "        NUM      0.969     0.912     0.939       272\n",
      "       PART      0.853     0.797     0.824       306\n",
      "       PRON      0.894     0.898     0.896       423\n",
      "      PROPN      0.833     0.875     0.854       576\n",
      "      PUNCT      0.998     0.999     0.998      2677\n",
      "      SCONJ      0.818     0.935     0.873       231\n",
      "        SYM      1.000     0.375     0.545        16\n",
      "       VERB      0.933     0.985     0.958      1359\n",
      "          X      0.958     0.786     0.864       117\n",
      "\n",
      "avg / total      0.948     0.948     0.946     14939\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def process_test_set(test_set):\n",
    "    sents = ''\n",
    "    for sent in test_set:\n",
    "        sent = ' '.join([w['form'].replace(' ', '') for w in sent])\n",
    "        sents += sent.strip() + '\\n'\n",
    "    return sents\n",
    "\n",
    "test_perc = process_test_set(test_set)\n",
    "pt = PerceptronTagger()\n",
    "perc_pred_labels = [word[1] for word in pt.tag(test_perc, tokenize=False)]\n",
    "print(classification_report(test_labels, perc_pred_labels, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.948\n"
     ]
    }
   ],
   "source": [
    "print(round(accuracy_score(test_labels, perc_pred_labels), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Цей таггер менш точний, але його натренована модель займає всього 4 мегабайти (порівняно з 60 мб для логістичної регресії)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Приклад на несхожому тексті (зі свіжих новин): порівняємо наші таггери і pymorphy2. (Я довго шукав такий текст, на якому були б хоч якісь грубі помилки таггерів, але поки не знайшов; є проблема зі словами, у яких є дефіс, але це проблема токенізації)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Уявімо VERB VERB VERB\n",
      ", PUNCT PUNCT None\n",
      "що SCONJ SCONJ CONJ\n",
      "завдання NOUN NOUN NOUN\n",
      "науковця NOUN NOUN NOUN\n",
      "- PUNCT PUNCT None\n",
      "не PART PART PRCL\n",
      "зробити VERB VERB VERB\n",
      "відкриття NOUN NOUN NOUN\n",
      ", PUNCT PUNCT None\n",
      "а CCONJ CCONJ CONJ\n",
      "створити VERB VERB VERB\n",
      "скульптуру NOUN NOUN NOUN\n",
      ". PUNCT PUNCT None\n",
      "Будь VERB VERB VERB\n",
      "- PUNCT PUNCT None\n",
      "яку DET DET NPRO\n",
      "і CCONJ CCONJ CONJ\n",
      "з ADP ADP PREP\n",
      "будь VERB VERB VERB\n",
      "- PUNCT PUNCT None\n",
      "чого PRON PRON NPRO\n",
      ". PUNCT PUNCT None\n",
      "Хороші ADJ ADJ ADJF\n",
      "вчені NOUN NOUN ADJF\n",
      ", PUNCT PUNCT None\n",
      "які DET DET NPRO\n",
      "бажають VERB VERB VERB\n",
      "співпрацювати VERB VERB VERB\n",
      "із ADP ADP PREP\n",
      "закордоном NOUN NOUN NOUN\n",
      ", PUNCT PUNCT None\n",
      "бути AUX AUX NOUN\n",
      "визнаними ADJ ADJ ADJF\n",
      "у ADP ADP PREP\n",
      "світі NOUN NOUN NOUN\n",
      ", PUNCT PUNCT None\n",
      "пишатися VERB VERB VERB\n",
      "своїм DET DET NPRO\n",
      "витвором NOUN NOUN NOUN\n",
      "- PUNCT PUNCT None\n",
      "докладатимуть VERB VERB VERB\n",
      "зусиль NOUN NOUN NOUN\n",
      ", PUNCT PUNCT None\n",
      "створюючи VERB VERB GRND\n",
      "витончені ADJ ADJ ADJF\n",
      "форми NOUN NOUN NOUN\n",
      "з ADP ADP PREP\n",
      "мармуру NOUN NOUN NOUN\n",
      ", PUNCT PUNCT None\n",
      "бронзи NOUN NOUN NOUN\n",
      "чи CCONJ CCONJ PRCL\n",
      ", PUNCT PUNCT None\n",
      "принаймні ADV ADV ADVB\n",
      ", PUNCT PUNCT None\n",
      "гіпсу NOUN NOUN NOUN\n",
      ". PUNCT PUNCT None\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"\n",
    "Уявімо, що завдання науковця - не зробити відкриття, а створити скульптуру.\n",
    "Будь-яку і з будь-чого. \n",
    "Хороші вчені, які бажають співпрацювати із закордоном, бути визнаними у світі, пишатися своїм витвором - докладатимуть зусиль, створюючи витончені форми з мармуру, бронзи чи, принаймні, гіпсу.\n",
    "\"\"\"\n",
    "sentences = [sent.strip(' \\n') for sent in text.split('\\n') if not sent == '']\n",
    "for sent in sentences:\n",
    "    tagged_sent = t.tag_sentence(sent)\n",
    "    perc_tagged_sent = pt.tag(sent)\n",
    "    words = [w[0] for w in tagged_sent]\n",
    "    tags1 = [w[1] for w in tagged_sent]\n",
    "    tags2 = [w[1] for w in perc_tagged_sent]\n",
    "    pytags = [morph.parse(w)[0].tag.POS for w in words]\n",
    "    for word, tag1, tag2, pytag in zip(words, tags1, tags2, pytags):\n",
    "        print(word, tag1, tag2, pytag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "Для багатьох задач такий поділ на частини мови не потрібний: AUX i VERB, DET i PRON, SYM i PUNCT можуть бути взаємозамінними. Можна подивитись на точність таггерів у такому випадку."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.968\n"
     ]
    }
   ],
   "source": [
    "def combine_tags(tag_list):\n",
    "    mapping = {'DET': 'PRON', 'SYM': 'PUNCT', \n",
    "               'AUX': 'VERB', 'PROPN': 'NOUN'}\n",
    "    return [mapping.get(tag, tag) for tag in tag_list]\n",
    "\n",
    "print(round(accuracy_score(combine_tags(test_labels), combine_tags(pred_labels)), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.967\n"
     ]
    }
   ],
   "source": [
    "print(round(accuracy_score(combine_tags(test_labels), combine_tags(perc_pred_labels)), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Покращення порівняно з бейзлайном (86%-89%) очевидне."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
